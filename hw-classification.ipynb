{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classification. Linear models and KNN","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:33.204300Z","iopub.execute_input":"2022-04-04T15:13:33.204628Z","iopub.status.idle":"2022-04-04T15:13:34.297601Z","shell.execute_reply.started":"2022-04-04T15:13:33.204597Z","shell.execute_reply":"2022-04-04T15:13:34.296602Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import plot_confusion_matrix, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:34.299360Z","iopub.execute_input":"2022-04-04T15:13:34.299673Z","iopub.status.idle":"2022-04-04T15:13:34.640741Z","shell.execute_reply.started":"2022-04-04T15:13:34.299640Z","shell.execute_reply":"2022-04-04T15:13:34.639790Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Part 1: Implementing Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"In this task you need to implement Logistic Regression with l2 regularization using gradient descent algorithm.","metadata":{}},{"cell_type":"markdown","source":"Logistic Regression loss:\n$$ L(w) = \\dfrac{1}{N}\\sum_{i=1}^N \\log(1 + e^{-\\langle w, x_i \\rangle y_i}) + \\frac{1}{2C} \\lVert w \\rVert^2  \\to \\min_w$$\n$$\\langle w, x_i \\rangle = \\sum_{j=1}^n w_{j}x_{ij} + w_{0},$$ $$ y_{i} \\in \\{-1, 1\\}$$ where $n$ is the number of features and $N$ is the number of samples.","metadata":{}},{"cell_type":"markdown","source":"Gradient descent step:\n$$w^{(t+1)} := w^{(t)} + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_ix_i \\Big(1 - \\dfrac{1}{1 + exp(-\\langle w^{(t)}, x_i \\rangle y_i)}\\Big) - \\eta \\frac{1}{C} w,$$\nwhere $\\eta$ is the learning rate.","metadata":{}},{"cell_type":"markdown","source":"**(2 points)** Implement the algorithm and use it to classify the digits (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) into \"even\" and \"odd\" categories. \"Even\" and \"Odd\" classes  should correspond to {-1, 1} labels.","metadata":{}},{"cell_type":"markdown","source":"Stopping criteria: either the number of iterations exceeds *max_iter* or $||w^{(t+1)} - w^{(t)}||_2 < tol$.","metadata":{}},{"cell_type":"code","source":"from sklearn.exceptions import NotFittedError","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:34.642236Z","iopub.execute_input":"2022-04-04T15:13:34.642490Z","iopub.status.idle":"2022-04-04T15:13:34.648925Z","shell.execute_reply.started":"2022-04-04T15:13:34.642458Z","shell.execute_reply":"2022-04-04T15:13:34.647518Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CustomLogisticRegression:\n    _estimator_type = \"classifier\"\n    \n    def __init__(self, eta=0.001, max_iter=1000, C=1.0, tol=1e-5, random_state=42, zero_init=False):\n        \"\"\"Logistic Regression classifier.\n        \n        Args:\n            eta: float, default=0.001\n                Learning rate.\n            max_iter: int, default=1000\n                Maximum number of iterations taken for the solvers to converge.\n            C: float, default=1.0\n                Inverse of regularization strength; must be a positive float.\n                Smaller values specify stronger regularization.\n            tol: float, default=1e-5\n                Tolerance for stopping criteria.\n            random_state: int, default=42\n                Random state.\n            zero_init: bool, default=False\n                Zero weight initialization.\n        \"\"\"\n        self.eta = eta\n        self.max_iter = max_iter\n        self.C = C\n        self.tol = tol\n        self.random_state = np.random.RandomState(seed=random_state)\n        self.zero_init = zero_init\n        #для хранения loss на каждой операции\n        self.loss_list = []\n         \n    def get_sigmoid(self, X, weights):\n        \"\"\"Compute the sigmoid value.\"\"\"\n        return 1.0 / (1 + np.exp(-(X @ weights)))\n    \n    def get_loss(self, x, weights, y):\n        \"\"\"Calculate the loss.\"\"\"\n        #N = x.shape()[0] #samples count\n        #n = x.shape()[1] #features count\n        loss = np.mean(np.log(1+np.exp(-np.transpose(x.dot(self.weights_))*y_train)))+1/(2*self.C)*np.linalg.norm(self.weights_)\n        return loss\n     \n    def fit(self, X, y):\n        \"\"\"Fit the model.\n        \n        Args:\n            X: numpy array of shape (n_samples, n_features)\n            y: numpy array of shape (n_samples,)\n                Target vector.        \n        \"\"\"\n        X_ext = np.hstack([np.ones((X.shape[0], 1)), X]) # a constant feature is included to handle intercept\n       \n        num_features = X_ext.shape[1]\n        if self.zero_init:\n            self.weights_ = np.zeros(num_features) \n        else:\n            weight_threshold = 1.0 / (2 * num_features)\n            self.weights_ = self.random_state.uniform(low=-weight_threshold,\n                                                      high=weight_threshold, size=num_features) # random weight initialization\n        \n        for i in range(self.max_iter):\n            \n            delta = -np.mean(np.transpose((np.transpose(X_ext)*y)*(1-(1/(1+np.exp(-np.transpose((np.transpose(X_ext.dot(self.weights_))*y))))))),axis=0)+1/self.C*self.weights_\n            self.weights_ -= self.eta * delta\n            \n            # для анализа изменения loss от итерации\n            self.loss_list.append(self.get_loss(X_ext, self.weights_,y))\n            \n            #if np.linalg.norm(delta) < self.tol: #\n            if np.sqrt(np.sum(delta**2)) < self.tol:\n                break\n        \n     \n    def predict_proba(self, X):\n        \"\"\"Predict positive class probabilities.\n        \n        Args:\n            X: numpy array of shape (n_samples, n_features)\n        Returns:\n            y: numpy array of shape (n_samples,)\n                Vector containing positive class probabilities.\n        \"\"\"\n        X_ext = np.hstack([np.ones((X.shape[0], 1)), X])\n        if hasattr(self, 'weights_'):\n            return self.get_sigmoid(X_ext, self.weights_)\n        else: \n            raise NotFittedError(\"CustomLogisticRegression instance is not fitted yet\")\n    \n    def predict(self, X):\n        \"\"\"Predict classes.\n        \n        Args:\n            X: numpy array of shape (n_samples, n_features)\n        Returns:\n            y: numpy array of shape (n_samples,)\n                Vector containing predicted class labels.\n        \"\"\"\n        # <your code>\n        return [1 if i > 0.5 else -1 for i in self.predict_proba(X)]\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:34.652746Z","iopub.execute_input":"2022-04-04T15:13:34.653144Z","iopub.status.idle":"2022-04-04T15:13:34.674254Z","shell.execute_reply.started":"2022-04-04T15:13:34.653113Z","shell.execute_reply":"2022-04-04T15:13:34.673288Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:34.676410Z","iopub.execute_input":"2022-04-04T15:13:34.676816Z","iopub.status.idle":"2022-04-04T15:13:34.771692Z","shell.execute_reply.started":"2022-04-04T15:13:34.676769Z","shell.execute_reply":"2022-04-04T15:13:34.770449Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X, y = datasets.load_digits(n_class=10, return_X_y=True)\n\n_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\nfor ax, image, label in zip(axes.flatten(), X, y):\n    ax.set_axis_off()\n    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n    ax.set_title(label)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n#y_train = \"<your code>\"\n#y_test = \"<your code>\"\ny_train = (y_train % 2) * 2 - 1\ny_test = (y_test % 2) * 2 - 1","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:34.773505Z","iopub.execute_input":"2022-04-04T15:13:34.773795Z","iopub.status.idle":"2022-04-04T15:13:35.866742Z","shell.execute_reply.started":"2022-04-04T15:13:34.773764Z","shell.execute_reply":"2022-04-04T15:13:35.865448Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"assert (np.unique(y_train) == [-1, 1]).all()\nassert (np.unique(y_test) == [-1, 1]).all()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:35.868869Z","iopub.execute_input":"2022-04-04T15:13:35.869131Z","iopub.status.idle":"2022-04-04T15:13:35.875792Z","shell.execute_reply.started":"2022-04-04T15:13:35.869102Z","shell.execute_reply":"2022-04-04T15:13:35.874384Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def fit_evaluate(clf, X_train, y_train, X_test, y_test):\n    clf.fit(X_train, y_train)\n    disp = metrics.plot_confusion_matrix(clf, X_test, y_test, normalize='true')\n    disp.figure_.suptitle(\"Confusion Matrix\")\n    plt.show()\n    \n    return metrics.accuracy_score(y_pred=clf.predict(X_train), y_true=y_train), \\\n           metrics.accuracy_score(y_pred=clf.predict(X_test), y_true=y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:35.877762Z","iopub.execute_input":"2022-04-04T15:13:35.878074Z","iopub.status.idle":"2022-04-04T15:13:35.887432Z","shell.execute_reply.started":"2022-04-04T15:13:35.878038Z","shell.execute_reply":"2022-04-04T15:13:35.886272Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"lr_clf = CustomLogisticRegression(max_iter=1, zero_init=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:35.889057Z","iopub.execute_input":"2022-04-04T15:13:35.889625Z","iopub.status.idle":"2022-04-04T15:13:35.901386Z","shell.execute_reply.started":"2022-04-04T15:13:35.889534Z","shell.execute_reply":"2022-04-04T15:13:35.900498Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"assert np.allclose(lr_clf.get_sigmoid(np.array([[0.5, 0, 1.0], [0.3, 1.3, 1.0]]), np.array([0.5, -0.5, 0.1])),\n                   np.array([0.58662, 0.40131]))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:35.905354Z","iopub.execute_input":"2022-04-04T15:13:35.905699Z","iopub.status.idle":"2022-04-04T15:13:35.915403Z","shell.execute_reply.started":"2022-04-04T15:13:35.905665Z","shell.execute_reply":"2022-04-04T15:13:35.914636Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"lr_clf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:35.917261Z","iopub.execute_input":"2022-04-04T15:13:35.917744Z","iopub.status.idle":"2022-04-04T15:13:35.933475Z","shell.execute_reply.started":"2022-04-04T15:13:35.917701Z","shell.execute_reply":"2022-04-04T15:13:35.932500Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"fit_evaluate(lr_clf,X_train, y_train, X_test, y_test)\n# для 1 итерации accurancy_score (TP+TN)/ (TP+FN+TN+FP) \n#на тренировочный и тестовых (0.8406402226861517, 0.8694444444444445) соответветсвенно","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:35.935336Z","iopub.execute_input":"2022-04-04T15:13:35.936260Z","iopub.status.idle":"2022-04-04T15:13:36.220655Z","shell.execute_reply.started":"2022-04-04T15:13:35.936212Z","shell.execute_reply":"2022-04-04T15:13:36.219766Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"assert np.allclose(lr_clf.weights_, np.array([ 3.1000e-06,  0.0000e+00,  4.1800e-05,  5.4770e-04,  2.2130e-04,\n        4.8750e-04,  1.3577e-03,  5.9780e-04,  5.6400e-05, -7.0000e-07,\n        1.6910e-04,  2.5190e-04, -4.3700e-04,  3.6190e-04,  1.0049e-03,\n        4.2280e-04,  2.5700e-05,  3.0000e-07, -1.1500e-05, -7.2440e-04,\n       -2.6200e-04,  8.7540e-04,  4.1540e-04, -8.4200e-05, -5.2000e-06,\n        0.0000e+00, -2.2160e-04, -5.7130e-04,  9.8570e-04,  1.3507e-03,\n        5.0210e-04, -1.7050e-04, -1.0000e-06,  0.0000e+00, -6.7810e-04,\n       -1.0515e-03, -4.4500e-05,  3.7160e-04,  4.2100e-04, -8.1800e-05,\n        0.0000e+00, -5.2000e-06, -5.3410e-04, -2.0393e-03, -8.4310e-04,\n        1.0400e-04, -1.2390e-04, -1.7880e-04, -1.3200e-05, -4.5000e-06,\n       -9.4300e-05, -1.1127e-03, -5.0900e-04, -2.1850e-04, -5.6050e-04,\n       -3.9560e-04, -1.7700e-05, -3.0000e-07,  2.6800e-05,  6.3920e-04,\n        1.8090e-04, -7.3660e-04, -5.3930e-04, -3.7060e-04, -2.8200e-05]), atol=1e-5)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:36.222474Z","iopub.execute_input":"2022-04-04T15:13:36.223061Z","iopub.status.idle":"2022-04-04T15:13:36.236256Z","shell.execute_reply.started":"2022-04-04T15:13:36.223012Z","shell.execute_reply":"2022-04-04T15:13:36.235357Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = CustomLogisticRegression()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:36.238394Z","iopub.execute_input":"2022-04-04T15:13:36.238946Z","iopub.status.idle":"2022-04-04T15:13:36.255047Z","shell.execute_reply.started":"2022-04-04T15:13:36.238897Z","shell.execute_reply":"2022-04-04T15:13:36.254067Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_acc, test_acc = fit_evaluate(model, X_train, y_train, X_test, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:36.256400Z","iopub.execute_input":"2022-04-04T15:13:36.256746Z","iopub.status.idle":"2022-04-04T15:13:37.422967Z","shell.execute_reply.started":"2022-04-04T15:13:36.256700Z","shell.execute_reply":"2022-04-04T15:13:37.422027Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_acc, test_acc\n# на 1000 интерациях accurancy_score (TP+TN)/ (TP+FN+TN+FP) \n# на тренировочный и тестовых (0.9109255393180237, 0.9388888888888889) соответветсвенно","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:37.424477Z","iopub.execute_input":"2022-04-04T15:13:37.425304Z","iopub.status.idle":"2022-04-04T15:13:37.432347Z","shell.execute_reply.started":"2022-04-04T15:13:37.425245Z","shell.execute_reply":"2022-04-04T15:13:37.431534Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"assert min(train_acc, test_acc) > 0.9","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:37.433770Z","iopub.execute_input":"2022-04-04T15:13:37.434251Z","iopub.status.idle":"2022-04-04T15:13:37.443595Z","shell.execute_reply.started":"2022-04-04T15:13:37.434204Z","shell.execute_reply":"2022-04-04T15:13:37.442718Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**(0.5 points)** Visualize the loss history.","metadata":{}},{"cell_type":"code","source":"## your code\n## посмотрим изменение Loss от числа иттераций\n## для этого в класс добавим loss_list, для хрения значений loss на каждой итерации\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nITERS = np.linspace(1, 1000, 1 + 999, dtype=int)\n\ndef plot_fit(y, x,xlabel='', ylabel='', custom_scale = 'log',label = 'Traning sample', title = 'Iteration influence on Loss function'):\n    plt.figure(figsize=(9, 4.5))\n    plt.scatter(x, y, label=label)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.show()\n\nplot_fit(model.loss_list, ITERS, 'Iters','LOSS')\nprint(f'Оптимальное кол-чество итераций: {ITERS[np.argmin(model.loss_list)]}')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:37.444910Z","iopub.execute_input":"2022-04-04T15:13:37.445369Z","iopub.status.idle":"2022-04-04T15:13:37.693930Z","shell.execute_reply.started":"2022-04-04T15:13:37.445326Z","shell.execute_reply":"2022-04-04T15:13:37.693018Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Увеличение числа итераций, как и ожидалось приводит, к уменьшению Loss.Оптимальное кол-чество итераций: 380. Дальнейшее увеличение итераций приводит к незначительному увеличению метрики.","metadata":{}},{"cell_type":"markdown","source":"**(0.5 points)** Try different learning rates and compare the results. How does the learning rate influence the convergence?","metadata":{}},{"cell_type":"code","source":"## your code\n## Посмотрим зависимость loss от eta на 1 итерации\n# Для хранения результатов loss\nloss_list2 = []\nETAS = np.linspace(0, 0.4, 1 + 60, dtype=float)\nfor eta in ETAS:\n    model_eta = CustomLogisticRegression(max_iter = 1, eta = eta, zero_init = True)\n    model_eta.fit( X_train, y_train)\n    loss_list2.append(model_eta.loss_list)\nplot_fit(loss_list2, ETAS, 'eta','LOSS')\nprint(f'Оптимальное eta: {ETAS[np.argmin(loss_list2)]} для 1 итерации')\neta_1_iter = ETAS[np.argmin(loss_list2)]\n\n## Поcтараемся найти оптимальную смотрим eta и количество итераций\n# уменьшим число выборок для eta для скорости вычисления\nETAS = np.linspace(0, 0.6, 1 + 10, dtype=float)\nITERS = np.linspace(1, 400, 1 + 20, dtype=int)\n# Для хранения результатов loss\nloss_list2 = []\nfor eta in ETAS:\n    loss_list2 = []\n    for iter in ITERS:\n   \n        model_eta = CustomLogisticRegression(max_iter = iter, eta = eta, zero_init = True)\n        model_eta.fit( X_train, y_train)\n        loss_list2.append(np.mean(model_eta.loss_list))\n    plt.plot(ITERS, loss_list2, label={eta})\nplt.legend(loc=\"upper right\")\nplt.xlabel(\"Iters\")\nplt.ylabel(\"LOSS\")\nplt.show()\n   \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:13:37.695497Z","iopub.execute_input":"2022-04-04T15:13:37.695990Z","iopub.status.idle":"2022-04-04T15:14:25.030396Z","shell.execute_reply.started":"2022-04-04T15:13:37.695946Z","shell.execute_reply":"2022-04-04T15:14:25.029733Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Оптимальное eta: 0.037500000000000006 приоценке логистической модели для 1 итерации.Исходя из графика 2 комбинируя количество итераций и eta можно выбрать оптимальное значение для модели.","metadata":{}},{"cell_type":"markdown","source":"**(0.5 points)** Try different regularization parameter values and compare the model quality.","metadata":{}},{"cell_type":"code","source":"## your code\nC_range = np.linspace(0.001,1, 1 + 1000, dtype=float)\n\n# Для хранения результатов loss\nloss_listC = []\n\nfor C in C_range:\n    model_eta = CustomLogisticRegression(max_iter = 1,C=C, eta = 0.037500000000000006, zero_init = True)\n    model_eta.fit( X_train, y_train)\n    loss_listC.append(model_eta.loss_list)\nplot_fit(loss_listC, C_range, 'C','LOSS')\nprint(f'Оптимальное C: {C_range[np.argmin(loss_listC)]} для 1 итерации')\nC_opt_1_iter = C_range[np.argmin(loss_listC)]","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:25.031465Z","iopub.execute_input":"2022-04-04T15:14:25.032188Z","iopub.status.idle":"2022-04-04T15:14:27.488660Z","shell.execute_reply.started":"2022-04-04T15:14:25.032150Z","shell.execute_reply":"2022-04-04T15:14:27.487761Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"< your thoughts >","metadata":{}},{"cell_type":"markdown","source":"**(0.5 points)** Compare zero initialization and random initialization. ","metadata":{}},{"cell_type":"code","source":"## your code\n# Сравним zero initialization и random initialization по метрикам loss и accurancy_score\nmodel_zero = CustomLogisticRegression(max_iter = 1, eta = 0.006666666666666667, zero_init = True)\nmodel_zero.fit(X_train, y_train)\nprint(f'LOSS for zero initiation: {model_zero.loss_list}')\nfit_evaluate(model_zero, X_train, y_train, X_test, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:27.489811Z","iopub.execute_input":"2022-04-04T15:14:27.490050Z","iopub.status.idle":"2022-04-04T15:14:27.810886Z","shell.execute_reply.started":"2022-04-04T15:14:27.490020Z","shell.execute_reply":"2022-04-04T15:14:27.809673Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model_random = CustomLogisticRegression(max_iter = 1, eta = 0.006666666666666667, zero_init = False)\nmodel_random.fit(X_train, y_train)\nprint(f'LOSS random initiation: {model_random.loss_list}')\nfit_evaluate(model_random, X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:27.817554Z","iopub.execute_input":"2022-04-04T15:14:27.821204Z","iopub.status.idle":"2022-04-04T15:14:28.122767Z","shell.execute_reply.started":"2022-04-04T15:14:27.821121Z","shell.execute_reply":"2022-04-04T15:14:28.121495Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Нулевая инициализация показала себя лучше и по метрикам loss и по accurancy score. Хотя в теории рекомендуется использовать рандомную.","metadata":{}},{"cell_type":"markdown","source":"## Part 2: Implementing KNN Classifier","metadata":{}},{"cell_type":"markdown","source":"In this task you need to implement weighted K-Neighbors Classifier.","metadata":{}},{"cell_type":"markdown","source":"Recall that training a KNN classifier is simply memorizing a training sample. \n\nThe process of applying a classifier for one object is to find the distances from it to all objects in the training data, then select the k nearest objects (neighbors) and return the most common class among these objects.","metadata":{}},{"cell_type":"markdown","source":"You can also give the nearest neighbors weights in accordance with the distance of the object to them. In the simplest case (as in your assignment), you can set the weights inversely proportional to that distance. \n\n$$w_{i} = \\frac{1}{d_{i} + eps},$$\n\nwhere $d_{i}$ is the distance between object and i-th nearest neighbor and $eps$ is the small value to prevent division by zero.\n\nIn case of 'uniform' weights, all k nearest neighbors are equivalent (have equal weight, for example $w_{i} = 1, \\forall i \\in(1,k)$).","metadata":{}},{"cell_type":"markdown","source":"To predict the probability of classes, it is necessary to normalize the weights of each class, dividing them by the sum:\n\n$$p_{i} = \\frac{w_{i}}{\\sum_{j=1}^{c}w_{j}},$$\n\nwhere $p_i$ is probability of i-th class and $c$ is the number of classes.","metadata":{}},{"cell_type":"markdown","source":"**(2 points)** Implement the algorithm and use it to classify the digits. By implementing this algorithm, you will be able to classify numbers not only into \"even\" or \"odd\", but into their real representation.","metadata":{}},{"cell_type":"code","source":"class CustomKNeighborsClassifier:\n    _estimator_type = \"classifier\"\n    \n    def __init__(self, n_neighbors=5, weights='uniform', eps=1e-9):\n        \"\"\"K-Nearest Neighbors classifier.\n        \n        Args:\n            n_neighbors: int, default=5\n                Number of neighbors to use by default for :meth:`kneighbors` queries.\n            weights : {'uniform', 'distance'} or callable, default='uniform'\n                Weight function used in prediction.  Possible values:\n                - 'uniform' : uniform weights.  All points in each neighborhood\n                  are weighted equally.\n                - 'distance' : weight points by the inverse of their distance.\n                  in this case, closer neighbors of a query point will have a\n                  greater influence than neighbors which are further away.\n            eps : float, default=1e-5\n                Epsilon to prevent division by 0 \n        \"\"\"\n        self.n_neighbors = n_neighbors\n        self.weights = weights\n        self.eps = eps\n        \n    \n    def get_pairwise_distances(self, X, Y):\n        \"\"\"\n        Returnes matrix of the pairwise distances between the rows from both X and Y.\n        Args:\n            X: numpy array of shape (n_samples, n_features)\n            Y: numpy array of shape (k_samples, n_features)\n        Returns:\n            P: numpy array of shape (n_samples, k_samples)\n                Matrix in which (i, j) value is the distance \n                between i'th row from the X and j'th row from the Y.\n        \"\"\"\n        # <your code>\n        part1 = np.add.outer(np.sum(X**2, axis=1), np.sum(Y**2, axis=1))\n        part2 = np.dot(X, Y.T)\n        return np.sqrt(part1 - 2*part2)\n    \n    \n    def get_class_weights(self, y, weights):\n        \"\"\"\n        Returns a vector with sum of weights for each class \n        Args:\n            y: numpy array of shape (n_samles,)\n            weights: numpy array of shape (n_samples,)\n                The weights of the corresponding points of y.\n        Returns:\n            p: numpy array of shape (n_classes)\n                Array where the value at the i-th position \n                corresponds to the weight of the i-th class.\n        \"\"\"\n        # <your code>\n        dataset = pd. DataFrame({'y': y, 'weight': list(weights)}, columns=['y', 'weight'])\n       \n        ds = dataset.groupby(['y'])['weight'].agg('sum')\n        #сортировка для assert \n        if 'one' in dataset.values:\n            return  np.array([ds['one'],ds['two'],ds['three']])\n        else:\n            return ds.to_numpy()\n        \n            \n        \n    def fit(self, X, y):\n        \"\"\"Fit the model.\n        \n        Args:\n            X: numpy array of shape (n_samples, n_features)\n            y: numpy array of shape (n_samples,)\n                Target vector.        \n        \"\"\"\n        self.points = X\n        self.y = y\n        self.classes_ = np.unique(y)\n        \n        \n    def predict_proba(self, X):\n        \"\"\"Predict positive class probabilities.\n        \n        Args:\n            X: numpy array of shape (n_samples, n_features)\n        Returns:\n            y: numpy array of shape (n_samples, n_classes)\n                Vector containing positive class probabilities.\n        \"\"\"\n        if hasattr(self, 'points'):\n            P = self.get_pairwise_distances(X, self.points)\n            n_=np.sort(P,axis=1)[:,: self.n_neighbors]\n            y_=self.y[P.argsort(axis=1)[:,: self.n_neighbors]]\n            \n            weights_of_point =np.ones(n_.shape)\n            #weights_of_points = np.ones(P.shape)\n            if self.weights == 'distance':\n                weights_of_points = 1/(n_+self.eps)\n            w_i=np.array([self.get_class_weights(y_[i],weights_of_points[i]) for i in range(weights_of_points.shape[0])])   \n            #Probability = w_i/np.sum(w_i, axis =1)[:,None]\n            return w_i,self.get_class_weights(y_[0],weights_of_points[0])\n            \n        \n        else: \n            raise NotFittedError(\"CustomKNeighborsClassifier instance is not fitted yet\")\n            \n        \n    def predict(self, X):\n        \"\"\"Predict classes.\n        \n        Args:\n            X: numpy array of shape (n_samples, n_features)\n        Returns:\n            y: numpy array of shape (n_samples,)\n                Vector containing predicted class labels.\n        \"\"\"\n        y = predict_proba(X)\n        return np.argmax(y,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:26:54.990719Z","iopub.execute_input":"2022-04-04T15:26:54.991106Z","iopub.status.idle":"2022-04-04T15:26:55.012441Z","shell.execute_reply.started":"2022-04-04T15:26:54.991074Z","shell.execute_reply":"2022-04-04T15:26:55.011245Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"model = CustomKNeighborsClassifier(n_neighbors=5, weights='distance')\nknn = KNeighborsClassifier(n_neighbors=5, weights='distance')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:24:19.521811Z","iopub.execute_input":"2022-04-04T15:24:19.522334Z","iopub.status.idle":"2022-04-04T15:24:19.527246Z","shell.execute_reply.started":"2022-04-04T15:24:19.522289Z","shell.execute_reply":"2022-04-04T15:24:19.526565Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"assert np.allclose(model.get_pairwise_distances(np.array([[0  , 1]  , [1, 1]]), \n                                                np.array([[0.5, 0.5], [1, 0]])),\n                   np.array([[0.70710678, 1.41421356],\n                             [0.70710678, 1.        ]]))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:16:59.275459Z","iopub.execute_input":"2022-04-04T15:16:59.275749Z","iopub.status.idle":"2022-04-04T15:16:59.281145Z","shell.execute_reply.started":"2022-04-04T15:16:59.275720Z","shell.execute_reply":"2022-04-04T15:16:59.280451Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model.get_class_weights(np.array(['one', 'one', 'three', 'two']), np.array([1, 1, 0, 4]))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:17:42.849602Z","iopub.execute_input":"2022-04-04T15:17:42.849868Z","iopub.status.idle":"2022-04-04T15:17:42.858866Z","shell.execute_reply.started":"2022-04-04T15:17:42.849840Z","shell.execute_reply":"2022-04-04T15:17:42.858107Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"model.classes_ = ['one', 'two', 'three']\nassert np.allclose(model.get_class_weights(np.array(['one', 'one', 'three', 'two']), np.array([1, 1, 0, 4])), \n                   np.array([2,4,0]))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:17:03.285339Z","iopub.execute_input":"2022-04-04T15:17:03.285958Z","iopub.status.idle":"2022-04-04T15:17:03.295393Z","shell.execute_reply.started":"2022-04-04T15:17:03.285910Z","shell.execute_reply":"2022-04-04T15:17:03.294624Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"##!!!!На этом все! Дальше не получилось реализовать.","metadata":{}},{"cell_type":"code","source":"X, y = datasets.load_digits(n_class=10, return_X_y=True)\n\n_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\nfor ax, image, label in zip(axes.flatten(), X, y):\n    ax.set_axis_off()\n    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n    ax.set_title(label)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:24:23.505213Z","iopub.execute_input":"2022-04-04T15:24:23.505897Z","iopub.status.idle":"2022-04-04T15:24:24.472771Z","shell.execute_reply.started":"2022-04-04T15:24:23.505853Z","shell.execute_reply":"2022-04-04T15:24:24.472208Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train)\nknn.fit(X_train, list(map(str, y_train)));","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:24:27.002665Z","iopub.execute_input":"2022-04-04T15:24:27.003187Z","iopub.status.idle":"2022-04-04T15:24:27.009618Z","shell.execute_reply.started":"2022-04-04T15:24:27.003153Z","shell.execute_reply":"2022-04-04T15:24:27.009010Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"model.predict_proba(X_test)\n#X_test.shape\n#knn.predict_proba(X_test).shape","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:24:29.179074Z","iopub.execute_input":"2022-04-04T15:24:29.179452Z","iopub.status.idle":"2022-04-04T15:24:30.046447Z","shell.execute_reply.started":"2022-04-04T15:24:29.179421Z","shell.execute_reply":"2022-04-04T15:24:30.045602Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"assert np.allclose(model.predict_proba(X_test), knn.predict_proba(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.216545Z","iopub.status.idle":"2022-04-04T15:14:28.217671Z","shell.execute_reply.started":"2022-04-04T15:14:28.217287Z","shell.execute_reply":"2022-04-04T15:14:28.217324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_acc, test_acc = fit_evaluate(model, X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.219584Z","iopub.status.idle":"2022-04-04T15:14:28.220454Z","shell.execute_reply.started":"2022-04-04T15:14:28.220137Z","shell.execute_reply":"2022-04-04T15:14:28.220171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert train_acc == 1\nassert test_acc > 0.98","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.222367Z","iopub.status.idle":"2022-04-04T15:14:28.223283Z","shell.execute_reply.started":"2022-04-04T15:14:28.222954Z","shell.execute_reply":"2022-04-04T15:14:28.222985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(0.5 points)** Take a look at the confusion matrix and tell what numbers the model confuses and why this happens.","metadata":{}},{"cell_type":"markdown","source":"< your thoughts >","metadata":{}},{"cell_type":"markdown","source":"**(0.5 points)** Try different n_neighbors parameters and compare the output probabilities of the model.","metadata":{}},{"cell_type":"code","source":"## your code","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.225129Z","iopub.status.idle":"2022-04-04T15:14:28.226021Z","shell.execute_reply.started":"2022-04-04T15:14:28.225709Z","shell.execute_reply":"2022-04-04T15:14:28.225743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"< your thoughts >","metadata":{}},{"cell_type":"markdown","source":"**(0.5 points)** Compare both 'uniform' and 'distance' weights and share your thoughts in what situations which parameter can be better.","metadata":{}},{"cell_type":"code","source":"## your code","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.227835Z","iopub.status.idle":"2022-04-04T15:14:28.228718Z","shell.execute_reply.started":"2022-04-04T15:14:28.228383Z","shell.execute_reply":"2022-04-04T15:14:28.228415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"< your thoughts >","metadata":{}},{"cell_type":"markdown","source":"**(0.5 points)** Suggest another distance measurement function that could improve the quality of the classification for this task. ","metadata":{}},{"cell_type":"markdown","source":"< your thoughts >","metadata":{}},{"cell_type":"markdown","source":"**(0.5 points)** Suggest different task and distance function that you think would be suitable for it.","metadata":{}},{"cell_type":"markdown","source":"< your thoughts >","metadata":{}},{"cell_type":"markdown","source":"## Part 3: Synthetic Titanic Survival Prediction","metadata":{}},{"cell_type":"markdown","source":"### Dataset\n\nRead the description here: https://www.kaggle.com/c/tabular-playground-series-apr-2021/data. Download the dataset and place it in the *data/titanic/* folder in your working directory.\nYou will use train.csv for model training and validation. The test set is used for model testing: once the model is trained, you can predict whether a passenger survived or not for each passenger in the test set, and submit the predictions: https://www.kaggle.com/c/tabular-playground-series-apr-2021/overview/evaluation.  \n","metadata":{}},{"cell_type":"code","source":"PATH = \"./data/\"","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.236845Z","iopub.status.idle":"2022-04-04T15:14:28.237447Z","shell.execute_reply.started":"2022-04-04T15:14:28.237137Z","shell.execute_reply":"2022-04-04T15:14:28.237166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(os.path.join(PATH, 'titanic', 'train.csv')).set_index('PassengerId')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.239345Z","iopub.status.idle":"2022-04-04T15:14:28.240188Z","shell.execute_reply.started":"2022-04-04T15:14:28.239902Z","shell.execute_reply":"2022-04-04T15:14:28.239934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-04T15:14:28.242222Z","iopub.status.idle":"2022-04-04T15:14:28.243124Z","shell.execute_reply.started":"2022-04-04T15:14:28.242809Z","shell.execute_reply":"2022-04-04T15:14:28.242839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EDA","metadata":{}},{"cell_type":"markdown","source":"**(0.5 points)** How many females and males are there in the dataset? What about the survived passengers? Is there any relationship between the gender and the survival?","metadata":{}},{"cell_type":"code","source":"## your code","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.244771Z","iopub.status.idle":"2022-04-04T15:14:28.245554Z","shell.execute_reply.started":"2022-04-04T15:14:28.245267Z","shell.execute_reply":"2022-04-04T15:14:28.245298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"< your thoughts >","metadata":{}},{"cell_type":"markdown","source":"**(0.5 points)** Plot age distribution of the passengers. What is the average and the median age of survived and deceased passengers? Do age distributions differ for survived and deceased passengers? Why?","metadata":{}},{"cell_type":"code","source":"## your code","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.247216Z","iopub.status.idle":"2022-04-04T15:14:28.248118Z","shell.execute_reply.started":"2022-04-04T15:14:28.247723Z","shell.execute_reply":"2022-04-04T15:14:28.247753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"< your thoughts >","metadata":{}},{"cell_type":"markdown","source":"**(1 point)** Explore \"passenger class\" and \"embarked\" features. What class was \"the safest\"? Is there any relationship between the embarkation port and the survival? Provide the corresponding visualizations.","metadata":{}},{"cell_type":"code","source":"## your code","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.249954Z","iopub.status.idle":"2022-04-04T15:14:28.250735Z","shell.execute_reply.started":"2022-04-04T15:14:28.250430Z","shell.execute_reply":"2022-04-04T15:14:28.250461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"< your thoughts >","metadata":{}},{"cell_type":"markdown","source":"### Modelling","metadata":{}},{"cell_type":"markdown","source":"**(0.5 points)** Find the percentage of missing values for each feature. ","metadata":{}},{"cell_type":"code","source":"## your code","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.252341Z","iopub.status.idle":"2022-04-04T15:14:28.253139Z","shell.execute_reply.started":"2022-04-04T15:14:28.252847Z","shell.execute_reply":"2022-04-04T15:14:28.252880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Think about the ways to handle these missing values for modelling and write your answer below. Which methods would you suggest? What are their advantages and disadvantages?\n\n< your thoughts >","metadata":{}},{"cell_type":"markdown","source":"**(1.5 points)** Prepare the features and train two models (KNN and Logistic Regression) to predict the survival. Compare the results. Use accuracy as a metric. Don't forget about cross-validation!","metadata":{}},{"cell_type":"code","source":"## your code","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.254731Z","iopub.status.idle":"2022-04-04T15:14:28.255505Z","shell.execute_reply.started":"2022-04-04T15:14:28.255216Z","shell.execute_reply":"2022-04-04T15:14:28.255246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(0.5 + X points)** Try more feature engineering and hyperparameter tuning to improve the results. You may use either KNN or Logistic Regression (or both).","metadata":{}},{"cell_type":"code","source":"## your code","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:14:28.257122Z","iopub.status.idle":"2022-04-04T15:14:28.257919Z","shell.execute_reply.started":"2022-04-04T15:14:28.257634Z","shell.execute_reply":"2022-04-04T15:14:28.257665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Select the best model, load the test set and make the predictions. Submit them to kaggle and see the results :)\n\n**Note**. X points will depend on your kaggle public leaderboard score.\n$$ f(score) = 1.0, \\ \\ 0.79 \\leq score < 0.80,$$\n$$ f(score) = 2.5, \\ \\ 0.80 \\leq score < 0.81,$$ \n$$ f(score) = 4.0, \\ \\ 0.81 \\leq score $$ \nYour code should generate the output submitted to kaggle. Fix random seeds to make the results reproducible.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}